{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Mounting the drive with google colab (testing purposes)"
      ],
      "metadata": {
        "id": "FjRd2KkhOOfg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoKXGwrdmCHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1fd8ffa-e3ef-42c2-b493-87b90046d853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# # my files are in 'labs/lab0-0'\n",
        "# !cp -r /content/drive/MyDrive/labs/MoedC/* .\n",
        "# # !pip install -r requirements.txt\n",
        "# # restart the runtime\n",
        "# import os\n",
        "# os._exit(00)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbd4feSJ1LLN",
        "outputId": "2497063f-1327-4d71-b6a2-51c26021ba83"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9657 sha256=100004a1e67e795f5286ae1397fb55a7541c1fe3782b49f32620f0e1d530bf64\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As we can see, we have different ids in here, each id is for another corpus, each corpus needed a research byitself, at first started with big corpus (the twitter data corpus found in github) named fileidBigData, then a small portion of it, fileidSmallData, after that I tried wikiData (Wikipedia sentences data got from kaggle), and then a small portion of this wikiData, after that i tried Shakespere story data found in google, then the cat in the hat story data by dr.Suess and last one was the one that I worked with it which was One fish Two fish by dr.Suess.**"
      ],
      "metadata": {
        "id": "NJcW44ZkNGQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# geah_filename = (\"https://docs.google.com/uc?export=download&id=1jLiHnzSW2Kcn93YFCNllLe3ry0MtSqRi\")\n",
        "# !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1jLiHnzSW2Kcn93YFCNllLe3ry0MtSqRi' -O Data.txt\n",
        "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1jLiHnzSW2Kcn93YFCNllLe3ry0MtSqRi' -O Data.txt\n",
        "fileidBigData= '1jLiHnzSW2Kcn93YFCNllLe3ry0MtSqRi'\n",
        "fileidSmallData = '1qC3UOL4LtxuD53DRiR3nq0eVflm0xWe7'\n",
        "wikiData = '1c_r1v-W3r_RdOTzn2HscUv7GeWDX1HKJ'\n",
        "smallWikiData = '1UQryOYY4-Ks2qBSHTamvPpjS_SXjwftw'\n",
        "checkData = '1WJFlnZLUMRB8WjVPRexkhQLShY24HuqL'\n",
        "shaksData = '1H5_sZ_Y5ax8-aU0Kuer4yXUqk76hlVm1'\n",
        "thecat = '11Ic4pLhnRFBe_woSGMZHaeJH1cpWESqx'\n",
        "thecatinthehat = '1XdGtXSdh-L0rDPZmCV9aXawF66n6jtHt'\n",
        "onefish = '1yoGeuk0dL-H-x7pFQFBaG9yKOp47OKiZ'\n",
        "filename= 'Data.txt'\n",
        "## WGET ##\n",
        "!wget --save-cookies cookies.txt 'https://docs.google.com/uc?export=download&id='$onefish -O- \\\n",
        "     | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1/p' > confirm.txt\n",
        "!wget --load-cookies cookies.txt -O $filename \\\n",
        "     'https://docs.google.com/uc?export=download&id='$onefish'&confirm='$(<confirm.txt)\n",
        "\n",
        "# os.makedirs(\"data1\", exist_ok = True)\n",
        "# https://drive.google.com/file/d/1jLiHnzSW2Kcn93YFCNllLe3ry0MtSqRi/view?usp=sharing\n",
        "# wget.download(geah_filename, out = \"data1/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cojIpAtWaV0G",
        "outputId": "b0529168-84af-437a-a25e-319defa64a0d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-26 19:44:36--  https://docs.google.com/uc?export=download&id=1yoGeuk0dL-H-x7pFQFBaG9yKOp47OKiZ\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.2.100, 142.251.2.101, 142.251.2.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.2.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-b0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/sm5uhta0gqq827t8p08ebcjuudbonml6/1687808625000/12878140382965408106/*/1yoGeuk0dL-H-x7pFQFBaG9yKOp47OKiZ?e=download&uuid=67a0c46a-c4a2-4283-8e8a-5e560a8ca80b [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-06-26 19:44:37--  https://doc-08-b0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/sm5uhta0gqq827t8p08ebcjuudbonml6/1687808625000/12878140382965408106/*/1yoGeuk0dL-H-x7pFQFBaG9yKOp47OKiZ?e=download&uuid=67a0c46a-c4a2-4283-8e8a-5e560a8ca80b\n",
            "Resolving doc-08-b0-docs.googleusercontent.com (doc-08-b0-docs.googleusercontent.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
            "Connecting to doc-08-b0-docs.googleusercontent.com (doc-08-b0-docs.googleusercontent.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5865 (5.7K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   5.73K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-26 19:44:37 (13.8 MB/s) - written to stdout [5865/5865]\n",
            "\n",
            "--2023-06-26 19:44:37--  https://docs.google.com/uc?export=download&id=1yoGeuk0dL-H-x7pFQFBaG9yKOp47OKiZ&confirm=\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.2.100, 142.251.2.101, 142.251.2.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.2.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-b0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/sm5uhta0gqq827t8p08ebcjuudbonml6/1687808625000/12878140382965408106/*/1yoGeuk0dL-H-x7pFQFBaG9yKOp47OKiZ?e=download&uuid=cbf237c6-c101-45e4-92c9-1e3cf95efafd [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-06-26 19:44:38--  https://doc-08-b0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/sm5uhta0gqq827t8p08ebcjuudbonml6/1687808625000/12878140382965408106/*/1yoGeuk0dL-H-x7pFQFBaG9yKOp47OKiZ?e=download&uuid=cbf237c6-c101-45e4-92c9-1e3cf95efafd\n",
            "Resolving doc-08-b0-docs.googleusercontent.com (doc-08-b0-docs.googleusercontent.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
            "Connecting to doc-08-b0-docs.googleusercontent.com (doc-08-b0-docs.googleusercontent.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5865 (5.7K) [text/plain]\n",
            "Saving to: ‘Data.txt’\n",
            "\n",
            "Data.txt            100%[===================>]   5.73K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-26 19:44:38 (52.8 MB/s) - ‘Data.txt’ saved [5865/5865]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the data\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "81qid-JfAuLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting a small portion of the data - One time thing"
      ],
      "metadata": {
        "id": "9mx-hRM5A0-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Count the number of lines in the dataset\n",
        "# with open('Data.txt', 'r', encoding='utf-8') as file:\n",
        "\n",
        "with open('Data.txt', 'r', encoding='latin-1') as file:\n",
        "    line_count = sum(1 for line in file)\n",
        "print(\"Number of lines in the dataset:\", line_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCySfF0TBBwT",
        "outputId": "3706d414-a6b6-460a-f66d-a946bf4baf00"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines in the dataset: 203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see above, the number of lines on the dataset is too large, therefore we are going to choose a small portion of 500 line of this data.\\\\\n",
        "In the following two cells, they were used when I checked the big corpuses, and i had to cut them off to use small portion of them."
      ],
      "metadata": {
        "id": "WPGSqgVwBFV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "\n",
        "# random.seed(42)  # Set a seed for reproducibility\n",
        "\n",
        "# # Step 2: Randomly select 200 lines from the dataset\n",
        "# selected_lines = random.sample(range(1, line_count + 1), 300)\n",
        "\n",
        "# # Step 3: Read the dataset file and save selected lines to small_data.txt\n",
        "# selected_sentences = []\n",
        "# # with open('Data.txt', 'r', encoding='utf-8') as input_file, \\\n",
        "# #         open('small_data.txt', 'w', encoding='utf-8') as output_file:\n",
        "# with open('Data.txt', 'r', encoding='latin-1') as input_file, \\\n",
        "#         open('small_data.txt', 'w', encoding='latin-1') as output_file:\n",
        "#     for line_number, line in enumerate(input_file, 1):\n",
        "#         if line_number in selected_lines:\n",
        "#             selected_sentences.append(line.strip())\n",
        "#             output_file.write(line)\n",
        "\n",
        "# print(\"Randomly selected sentences have been saved to small_data.txt.\")\n"
      ],
      "metadata": {
        "id": "CdRRsrYnd97r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(selected_sentences[:10])"
      ],
      "metadata": {
        "id": "AIMKlz3YBi_s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now I downloaded the small_data.txt and saved it on google drive, so in the future runs I can immediately download it and use it without doing the preprocessing again!**"
      ],
      "metadata": {
        "id": "RUeJHE8bCIN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing part on the small portion of the data"
      ],
      "metadata": {
        "id": "PP0P0YFCCkGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we downloaded the data again (the second cell), but we now took the small portion of data, the 500 lines and saved them on the Data.txt file, and we moving forward to preprocess them:"
      ],
      "metadata": {
        "id": "TDO_4_-mC5xU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import math\n",
        "import random\n",
        "import re\n",
        "import wget\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "from sys import getsizeof\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt', quiet=True) # this module is used to tokenize the text\n",
        "\n",
        "# Set random seeds\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "\n",
        "\n",
        "# Some utilities to manipulate the corpus\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"Strips #comments and empty lines from a string\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for line in text.split(\"\\n\"):\n",
        "        line = line.strip()              # trim whitespace\n",
        "        line = re.sub('#.*$', '', line)  # trim comments\n",
        "        if line != '':                   # drop blank lines\n",
        "            result.append(line)\n",
        "    return result\n",
        "\n",
        "def nltk_normpunc_tokenize(str):\n",
        "    return nltk.tokenize.word_tokenize(str.lower())\n",
        "\n",
        "\n",
        "def geah_tokenize(lines):\n",
        "    \"\"\"Specialized tokenizer for GEaH corpus handling speaker IDs\"\"\"\n",
        "    result = []\n",
        "    for line in lines:\n",
        "        # tokenize\n",
        "        tokens = nltk_normpunc_tokenize(line)\n",
        "        # revert the speaker ID token\n",
        "        # if tokens[0] == \"sam\":\n",
        "        #     tokens[0] = \"SAM:\"\n",
        "        # elif tokens[0] == \"guy\":\n",
        "        #     tokens[0] = \"GUY:\"\n",
        "        # else:\n",
        "        #     raise ValueError(\"format problem - bad speaker ID\")\n",
        "        # add a start of sentence token\n",
        "        result += [\"<s>\"] + tokens\n",
        "    return result\n",
        "\n",
        "def postprocess(tokens):\n",
        "    \"\"\"Converts `tokens` to a string with one sentence per line\"\"\"\n",
        "    return ' '.join(tokens)\\\n",
        "              .replace(\"<s> \", \"\\n\")\n",
        "\n",
        "\n",
        "def split(list, portions, offset):\n",
        "    \"\"\"Splits `list` into a \"large\" and a \"small\" part, returning them as a pair.\n",
        "\n",
        "    The two parts are formed by partitioning `list` into `portions` disjoint pieces.\n",
        "    The small part is the piece at index `offset`; the large part is the remainder.\n",
        "    \"\"\"\n",
        "    return ([list[i] for i in range(0, len(list)) if i % portions != offset],\n",
        "            [list[i] for i in range(0, len(list)) if i % portions == offset])\n",
        "\n",
        "with open(\"Data.txt\", 'r') as fin:\n",
        "    lines = preprocess(fin.read())\n",
        "    train_lines, test_lines = split(lines, 12, 0)\n",
        "    train_tokens = geah_tokenize(train_lines)\n",
        "    test_tokens = geah_tokenize(test_lines)"
      ],
      "metadata": {
        "id": "Dr8z1FeTnUQz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = list(set(train_tokens))\n",
        "print(len(vocabulary))\n",
        "vocabulary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyEaqucwE5sZ",
        "outputId": "a8f18207-8ee1-4c20-d5aa-77c20e4b732d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "292\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['called',\n",
              " 'things',\n",
              " 'oh',\n",
              " 'many',\n",
              " 'then',\n",
              " 'does',\n",
              " 'these',\n",
              " 'with',\n",
              " 'fun',\n",
              " 'would',\n",
              " 'mother',\n",
              " 'bird',\n",
              " 'story',\n",
              " 'it',\n",
              " 'near',\n",
              " 'nook',\n",
              " 'glad',\n",
              " 'never',\n",
              " 'is',\n",
              " 'girls',\n",
              " 'a',\n",
              " 'sleep',\n",
              " 'sits',\n",
              " 'like',\n",
              " 'they',\n",
              " 'his',\n",
              " 'teeth',\n",
              " 'something',\n",
              " 'hook',\n",
              " 'think',\n",
              " 'comb',\n",
              " 'funny',\n",
              " 'moon',\n",
              " 'quiet',\n",
              " 'go',\n",
              " 'gack',\n",
              " 'ride',\n",
              " 'what',\n",
              " 'up',\n",
              " 'feet',\n",
              " 'made',\n",
              " 'seven',\n",
              " 'yop',\n",
              " 'wet',\n",
              " 'your',\n",
              " 'fish',\n",
              " 'have',\n",
              " 'four',\n",
              " 'ear',\n",
              " 'saw',\n",
              " 'car',\n",
              " 'on',\n",
              " 'put',\n",
              " 'zans',\n",
              " 'should',\n",
              " 'fly',\n",
              " 'anything',\n",
              " 'grow',\n",
              " 'socks',\n",
              " 'gone',\n",
              " 'shoe',\n",
              " 'we',\n",
              " 'pets',\n",
              " 'if',\n",
              " 'ned',\n",
              " 'light',\n",
              " 'and',\n",
              " 'he',\n",
              " 'fat',\n",
              " 'big',\n",
              " 'took',\n",
              " 'right',\n",
              " 'over',\n",
              " 'pink',\n",
              " 'again',\n",
              " 'why',\n",
              " 'i',\n",
              " 'my',\n",
              " 'bet',\n",
              " 'good',\n",
              " 'sad',\n",
              " 'jump',\n",
              " 'to',\n",
              " 'cow',\n",
              " 'know',\n",
              " 'way',\n",
              " 'tell',\n",
              " 'every',\n",
              " 'yellow',\n",
              " 'work',\n",
              " 'did',\n",
              " 'wish',\n",
              " 'five',\n",
              " 'where',\n",
              " 'can',\n",
              " 'kind',\n",
              " 'as',\n",
              " 'comes',\n",
              " 'hand',\n",
              " 'fingers',\n",
              " 'grows',\n",
              " 'day',\n",
              " 'hello',\n",
              " 'upon',\n",
              " 'lot',\n",
              " 'head',\n",
              " 'cats',\n",
              " 'foot',\n",
              " 'only',\n",
              " 'home',\n",
              " 'make',\n",
              " 'slow',\n",
              " 'ca',\n",
              " 'back',\n",
              " 'black',\n",
              " 'well',\n",
              " 'far',\n",
              " 'all',\n",
              " 'gump',\n",
              " 'box',\n",
              " '<s>',\n",
              " 'told',\n",
              " 'thing',\n",
              " 'fear',\n",
              " 'thin',\n",
              " 'see',\n",
              " 'so',\n",
              " 'me',\n",
              " 'at',\n",
              " 'eight',\n",
              " 'has',\n",
              " 'am',\n",
              " 'hear',\n",
              " 'mouse',\n",
              " 'ying',\n",
              " 'joe',\n",
              " 'him',\n",
              " 'you',\n",
              " 'blue',\n",
              " 'top',\n",
              " 'was',\n",
              " 'said',\n",
              " 'from',\n",
              " 'today',\n",
              " 'littlecar',\n",
              " 'bed',\n",
              " 'walked',\n",
              " 'goodbye',\n",
              " 'get',\n",
              " 'ish',\n",
              " 'just',\n",
              " 'very',\n",
              " 'hat',\n",
              " '!',\n",
              " 'gold',\n",
              " 'cold',\n",
              " 'that',\n",
              " 'too',\n",
              " 'wave',\n",
              " 'about',\n",
              " 'everywhere',\n",
              " 'zeds',\n",
              " 'them',\n",
              " '.',\n",
              " 'the',\n",
              " 'yink',\n",
              " 'bad',\n",
              " 'ten',\n",
              " 'cut',\n",
              " 'wink',\n",
              " 'swish',\n",
              " 'ever',\n",
              " 'wump',\n",
              " 'not',\n",
              " 'of',\n",
              " 'sings',\n",
              " 'likes',\n",
              " 'clark',\n",
              " 'when',\n",
              " '``',\n",
              " 'night',\n",
              " 'need',\n",
              " '?',\n",
              " 'their',\n",
              " ':',\n",
              " 'left',\n",
              " 'long',\n",
              " 'ring',\n",
              " 'read',\n",
              " 'no',\n",
              " 'heads',\n",
              " 'bike',\n",
              " 'pet',\n",
              " 'mr.',\n",
              " 'more',\n",
              " 'but',\n",
              " 'dad',\n",
              " 'ink',\n",
              " 'yell',\n",
              " 'pop',\n",
              " 'some',\n",
              " 'haircut',\n",
              " 'call',\n",
              " 'our',\n",
              " 'cans',\n",
              " 'here',\n",
              " 'brush',\n",
              " 'gox',\n",
              " 'tomorrow',\n",
              " 'mike',\n",
              " 'there',\n",
              " 'look',\n",
              " 'bump',\n",
              " 'house',\n",
              " 'hop',\n",
              " 'run',\n",
              " 'hold',\n",
              " 'new',\n",
              " 'one',\n",
              " 'name',\n",
              " 'star',\n",
              " 'kite',\n",
              " 'us',\n",
              " 'live',\n",
              " 'hills',\n",
              " 'let',\n",
              " 'high',\n",
              " 'zeep',\n",
              " 'please',\n",
              " 'dish',\n",
              " 'hair',\n",
              " 'six',\n",
              " 'dear',\n",
              " 'try',\n",
              " 'finger',\n",
              " 'three',\n",
              " \"n't\",\n",
              " 'how',\n",
              " 'open',\n",
              " 'fast',\n",
              " \"''\",\n",
              " 'town',\n",
              " 'for',\n",
              " 'ask',\n",
              " 'man',\n",
              " 'another',\n",
              " 'had',\n",
              " 'take',\n",
              " 'milk',\n",
              " 'time',\n",
              " 'game',\n",
              " 'are',\n",
              " 'be',\n",
              " 'do',\n",
              " ',',\n",
              " 'two',\n",
              " 'who',\n",
              " 'out',\n",
              " 'in',\n",
              " 'down',\n",
              " 'yes',\n",
              " '...',\n",
              " 'sheep',\n",
              " 'drink',\n",
              " 'wire',\n",
              " 'walk',\n",
              " 'hump',\n",
              " 'come',\n",
              " ';',\n",
              " 'nine',\n",
              " 'old',\n",
              " 'by',\n",
              " 'now',\n",
              " 'sun',\n",
              " 'red',\n",
              " 'off',\n",
              " 'sing',\n",
              " 'sticks',\n",
              " 'may',\n",
              " 'stick',\n",
              " 'low',\n",
              " 'eleven',\n",
              " 'hot',\n",
              " 'say',\n",
              " 'cook',\n",
              " 'must',\n",
              " 'little',\n",
              " 'play',\n",
              " 'this',\n",
              " 'book',\n",
              " 'will',\n",
              " 'sit']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating n-grams"
      ],
      "metadata": {
        "id": "B8ywFVKa0h4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import math\n",
        "import random\n",
        "import re\n",
        "import wget\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "from sys import getsizeof\n",
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n"
      ],
      "metadata": {
        "id": "907vAtl70eyH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2378f4c8-9fac-4a34-e653-23e3bc3a5b91"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def all_ngrams(vocabulary, n):\n",
        "  return itertools.product(vocabulary,repeat=n)"
      ],
      "metadata": {
        "id": "8yPvwrNIHRsd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ngrams(tokens, n):\n",
        "    return [tuple(tokens[i : i + n])\n",
        "            for i in range(0, len(tokens) - n + 1)]"
      ],
      "metadata": {
        "id": "Iu6uGkrNHbXQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_tokens[:6])\n",
        "print(ngrams(train_tokens[:6], 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQ6njX7hHmrg",
        "outputId": "88019690-e939-45b2-cefb-71cedeceb30d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'black', 'fish', ',', 'blue', 'fish']\n",
            "[('<s>', 'black', 'fish'), ('black', 'fish', ','), ('fish', ',', 'blue'), (',', 'blue', 'fish')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Counting n-grams"
      ],
      "metadata": {
        "id": "FpZ9gMy2Hy_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ngram_counts(vocabulary, tokens, n):\n",
        "    context_dict = defaultdict(lambda: defaultdict(int))\n",
        "    # zero all ngrams\n",
        "    for context in all_ngrams(vocabulary, n - 1):\n",
        "        for target in vocabulary:\n",
        "            context_dict[context][target] = 0\n",
        "    # add counts for attested ngrams\n",
        "    for ngram, count in Counter(ngrams(tokens, n)).items():\n",
        "        context_dict[ngram[:-1]][ngram[-1]] = count\n",
        "    return context_dict"
      ],
      "metadata": {
        "id": "NL60A_M6HuGe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_counts = ngram_counts(vocabulary, train_tokens, 1)"
      ],
      "metadata": {
        "id": "-3uoZN1jIQ1b"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_counts = ngram_counts(vocabulary, train_tokens, 2)"
      ],
      "metadata": {
        "id": "yd5LlTkeIVII"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_counts = ngram_counts(vocabulary, train_tokens, 3)"
      ],
      "metadata": {
        "id": "E29vu3u_IXkQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate total counts of tokens, unigrams, bigrams, and trigrams\n",
        "token_count = len(train_tokens)\n",
        "unigram_count = sum(len(unigram_counts[cntxt]) for cntxt in unigram_counts)\n",
        "bigram_count = sum(len(bigram_counts[cntxt]) for cntxt in bigram_counts)\n",
        "trigram_count = sum(len(trigram_counts[cntxt]) for cntxt in trigram_counts)\n",
        "# Report on the totals\n",
        "print(f\"Tokens:   {token_count:6}\\n\"\n",
        "      f\"Unigrams: {unigram_count:6}\\n\"\n",
        "      f\"Bigrams:  {bigram_count:6}\\n\"\n",
        "      f\"Trigrams: {trigram_count:6}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaCeTorhIaXc",
        "outputId": "9aa77062-b0d0-4d7f-d855-575971ebef97"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:     1610\n",
            "Unigrams:    292\n",
            "Bigrams:   85264\n",
            "Trigrams: 24897088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating n-gram probabilities"
      ],
      "metadata": {
        "id": "gMCk5YoARSKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ngram_model(ngram_counts):\n",
        "    \"\"\"Returns an n-gram probability model calculated by normalizing the\n",
        "       provided `ngram-counts` dictionary\n",
        "    \"\"\"\n",
        "    normalized_counts = defaultdict(lambda: defaultdict(int))\n",
        "    for key, value in ngram_counts.items():\n",
        "      for subsequent, count in value.items():\n",
        "        normalized_counts[key][subsequent] = count\n",
        "\n",
        "    for value in normalized_counts.values():\n",
        "      total_count = sum(value.values())\n",
        "      for subsequent, count in value.items():\n",
        "        value[subsequent] = count / total_count if total_count != 0 else 0\n",
        "\n",
        "    return normalized_counts"
      ],
      "metadata": {
        "id": "maXpk9X9Pcx9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_model = ngram_model(unigram_counts)\n",
        "bigram_model = ngram_model(bigram_counts)\n",
        "trigram_model = ngram_model(trigram_counts)"
      ],
      "metadata": {
        "id": "G1Vm5TsMRsKV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Tokens:   {getsizeof(train_tokens):6}\\n\"\n",
        "      f\"Unigrams: {getsizeof(unigram_model):6}\\n\"\n",
        "      f\"Bigrams:  {getsizeof(bigram_model):6}\\n\"\n",
        "      f\"Trigrams: {getsizeof(trigram_model):6}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnOfHNB4R87J",
        "outputId": "7ec39037-25a0-4920-bc04-121e1f32e21c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:    13176\n",
            "Unigrams:    240\n",
            "Bigrams:    9320\n",
            "Trigrams: 2621544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_model[('will', 'not')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTUkGgKgSmsc",
        "outputId": "85661b8f-73d3-4faa-908f-5a2a81c75d25"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'called': 0.0,\n",
              "             'things': 0.0,\n",
              "             'oh': 0.0,\n",
              "             'many': 0.0,\n",
              "             'then': 0.0,\n",
              "             'does': 0.0,\n",
              "             'these': 0.0,\n",
              "             'with': 0.0,\n",
              "             'fun': 0.0,\n",
              "             'would': 0.0,\n",
              "             'mother': 0.0,\n",
              "             'bird': 0.0,\n",
              "             'story': 0.0,\n",
              "             'it': 0.0,\n",
              "             'near': 0.0,\n",
              "             'nook': 0.0,\n",
              "             'glad': 0.0,\n",
              "             'never': 0.0,\n",
              "             'is': 0.0,\n",
              "             'girls': 0.0,\n",
              "             'a': 0.0,\n",
              "             'sleep': 0.0,\n",
              "             'sits': 0.0,\n",
              "             'like': 0.0,\n",
              "             'they': 0.0,\n",
              "             'his': 0.0,\n",
              "             'teeth': 0.0,\n",
              "             'something': 0.0,\n",
              "             'hook': 0.0,\n",
              "             'think': 0.0,\n",
              "             'comb': 0.0,\n",
              "             'funny': 0.0,\n",
              "             'moon': 0.0,\n",
              "             'quiet': 0.0,\n",
              "             'go': 0.0,\n",
              "             'gack': 0.0,\n",
              "             'ride': 0.0,\n",
              "             'what': 0.0,\n",
              "             'up': 0.0,\n",
              "             'feet': 0.0,\n",
              "             'made': 0.0,\n",
              "             'seven': 0.0,\n",
              "             'yop': 0.0,\n",
              "             'wet': 0.0,\n",
              "             'your': 0.0,\n",
              "             'fish': 0.0,\n",
              "             'have': 1.0,\n",
              "             'four': 0.0,\n",
              "             'ear': 0.0,\n",
              "             'saw': 0.0,\n",
              "             'car': 0.0,\n",
              "             'on': 0.0,\n",
              "             'put': 0.0,\n",
              "             'zans': 0.0,\n",
              "             'should': 0.0,\n",
              "             'fly': 0.0,\n",
              "             'anything': 0.0,\n",
              "             'grow': 0.0,\n",
              "             'socks': 0.0,\n",
              "             'gone': 0.0,\n",
              "             'shoe': 0.0,\n",
              "             'we': 0.0,\n",
              "             'pets': 0.0,\n",
              "             'if': 0.0,\n",
              "             'ned': 0.0,\n",
              "             'light': 0.0,\n",
              "             'and': 0.0,\n",
              "             'he': 0.0,\n",
              "             'fat': 0.0,\n",
              "             'big': 0.0,\n",
              "             'took': 0.0,\n",
              "             'right': 0.0,\n",
              "             'over': 0.0,\n",
              "             'pink': 0.0,\n",
              "             'again': 0.0,\n",
              "             'why': 0.0,\n",
              "             'i': 0.0,\n",
              "             'my': 0.0,\n",
              "             'bet': 0.0,\n",
              "             'good': 0.0,\n",
              "             'sad': 0.0,\n",
              "             'jump': 0.0,\n",
              "             'to': 0.0,\n",
              "             'cow': 0.0,\n",
              "             'know': 0.0,\n",
              "             'way': 0.0,\n",
              "             'tell': 0.0,\n",
              "             'every': 0.0,\n",
              "             'yellow': 0.0,\n",
              "             'work': 0.0,\n",
              "             'did': 0.0,\n",
              "             'wish': 0.0,\n",
              "             'five': 0.0,\n",
              "             'where': 0.0,\n",
              "             'can': 0.0,\n",
              "             'kind': 0.0,\n",
              "             'as': 0.0,\n",
              "             'comes': 0.0,\n",
              "             'hand': 0.0,\n",
              "             'fingers': 0.0,\n",
              "             'grows': 0.0,\n",
              "             'day': 0.0,\n",
              "             'hello': 0.0,\n",
              "             'upon': 0.0,\n",
              "             'lot': 0.0,\n",
              "             'head': 0.0,\n",
              "             'cats': 0.0,\n",
              "             'foot': 0.0,\n",
              "             'only': 0.0,\n",
              "             'home': 0.0,\n",
              "             'make': 0.0,\n",
              "             'slow': 0.0,\n",
              "             'ca': 0.0,\n",
              "             'back': 0.0,\n",
              "             'black': 0.0,\n",
              "             'well': 0.0,\n",
              "             'far': 0.0,\n",
              "             'all': 0.0,\n",
              "             'gump': 0.0,\n",
              "             'box': 0.0,\n",
              "             '<s>': 0.0,\n",
              "             'told': 0.0,\n",
              "             'thing': 0.0,\n",
              "             'fear': 0.0,\n",
              "             'thin': 0.0,\n",
              "             'see': 0.0,\n",
              "             'so': 0.0,\n",
              "             'me': 0.0,\n",
              "             'at': 0.0,\n",
              "             'eight': 0.0,\n",
              "             'has': 0.0,\n",
              "             'am': 0.0,\n",
              "             'hear': 0.0,\n",
              "             'mouse': 0.0,\n",
              "             'ying': 0.0,\n",
              "             'joe': 0.0,\n",
              "             'him': 0.0,\n",
              "             'you': 0.0,\n",
              "             'blue': 0.0,\n",
              "             'top': 0.0,\n",
              "             'was': 0.0,\n",
              "             'said': 0.0,\n",
              "             'from': 0.0,\n",
              "             'today': 0.0,\n",
              "             'littlecar': 0.0,\n",
              "             'bed': 0.0,\n",
              "             'walked': 0.0,\n",
              "             'goodbye': 0.0,\n",
              "             'get': 0.0,\n",
              "             'ish': 0.0,\n",
              "             'just': 0.0,\n",
              "             'very': 0.0,\n",
              "             'hat': 0.0,\n",
              "             '!': 0.0,\n",
              "             'gold': 0.0,\n",
              "             'cold': 0.0,\n",
              "             'that': 0.0,\n",
              "             'too': 0.0,\n",
              "             'wave': 0.0,\n",
              "             'about': 0.0,\n",
              "             'everywhere': 0.0,\n",
              "             'zeds': 0.0,\n",
              "             'them': 0.0,\n",
              "             '.': 0.0,\n",
              "             'the': 0.0,\n",
              "             'yink': 0.0,\n",
              "             'bad': 0.0,\n",
              "             'ten': 0.0,\n",
              "             'cut': 0.0,\n",
              "             'wink': 0.0,\n",
              "             'swish': 0.0,\n",
              "             'ever': 0.0,\n",
              "             'wump': 0.0,\n",
              "             'not': 0.0,\n",
              "             'of': 0.0,\n",
              "             'sings': 0.0,\n",
              "             'likes': 0.0,\n",
              "             'clark': 0.0,\n",
              "             'when': 0.0,\n",
              "             '``': 0.0,\n",
              "             'night': 0.0,\n",
              "             'need': 0.0,\n",
              "             '?': 0.0,\n",
              "             'their': 0.0,\n",
              "             ':': 0.0,\n",
              "             'left': 0.0,\n",
              "             'long': 0.0,\n",
              "             'ring': 0.0,\n",
              "             'read': 0.0,\n",
              "             'no': 0.0,\n",
              "             'heads': 0.0,\n",
              "             'bike': 0.0,\n",
              "             'pet': 0.0,\n",
              "             'mr.': 0.0,\n",
              "             'more': 0.0,\n",
              "             'but': 0.0,\n",
              "             'dad': 0.0,\n",
              "             'ink': 0.0,\n",
              "             'yell': 0.0,\n",
              "             'pop': 0.0,\n",
              "             'some': 0.0,\n",
              "             'haircut': 0.0,\n",
              "             'call': 0.0,\n",
              "             'our': 0.0,\n",
              "             'cans': 0.0,\n",
              "             'here': 0.0,\n",
              "             'brush': 0.0,\n",
              "             'gox': 0.0,\n",
              "             'tomorrow': 0.0,\n",
              "             'mike': 0.0,\n",
              "             'there': 0.0,\n",
              "             'look': 0.0,\n",
              "             'bump': 0.0,\n",
              "             'house': 0.0,\n",
              "             'hop': 0.0,\n",
              "             'run': 0.0,\n",
              "             'hold': 0.0,\n",
              "             'new': 0.0,\n",
              "             'one': 0.0,\n",
              "             'name': 0.0,\n",
              "             'star': 0.0,\n",
              "             'kite': 0.0,\n",
              "             'us': 0.0,\n",
              "             'live': 0.0,\n",
              "             'hills': 0.0,\n",
              "             'let': 0.0,\n",
              "             'high': 0.0,\n",
              "             'zeep': 0.0,\n",
              "             'please': 0.0,\n",
              "             'dish': 0.0,\n",
              "             'hair': 0.0,\n",
              "             'six': 0.0,\n",
              "             'dear': 0.0,\n",
              "             'try': 0.0,\n",
              "             'finger': 0.0,\n",
              "             'three': 0.0,\n",
              "             \"n't\": 0.0,\n",
              "             'how': 0.0,\n",
              "             'open': 0.0,\n",
              "             'fast': 0.0,\n",
              "             \"''\": 0.0,\n",
              "             'town': 0.0,\n",
              "             'for': 0.0,\n",
              "             'ask': 0.0,\n",
              "             'man': 0.0,\n",
              "             'another': 0.0,\n",
              "             'had': 0.0,\n",
              "             'take': 0.0,\n",
              "             'milk': 0.0,\n",
              "             'time': 0.0,\n",
              "             'game': 0.0,\n",
              "             'are': 0.0,\n",
              "             'be': 0.0,\n",
              "             'do': 0.0,\n",
              "             ',': 0.0,\n",
              "             'two': 0.0,\n",
              "             'who': 0.0,\n",
              "             'out': 0.0,\n",
              "             'in': 0.0,\n",
              "             'down': 0.0,\n",
              "             'yes': 0.0,\n",
              "             '...': 0.0,\n",
              "             'sheep': 0.0,\n",
              "             'drink': 0.0,\n",
              "             'wire': 0.0,\n",
              "             'walk': 0.0,\n",
              "             'hump': 0.0,\n",
              "             'come': 0.0,\n",
              "             ';': 0.0,\n",
              "             'nine': 0.0,\n",
              "             'old': 0.0,\n",
              "             'by': 0.0,\n",
              "             'now': 0.0,\n",
              "             'sun': 0.0,\n",
              "             'red': 0.0,\n",
              "             'off': 0.0,\n",
              "             'sing': 0.0,\n",
              "             'sticks': 0.0,\n",
              "             'may': 0.0,\n",
              "             'stick': 0.0,\n",
              "             'low': 0.0,\n",
              "             'eleven': 0.0,\n",
              "             'hot': 0.0,\n",
              "             'say': 0.0,\n",
              "             'cook': 0.0,\n",
              "             'must': 0.0,\n",
              "             'little': 0.0,\n",
              "             'play': 0.0,\n",
              "             'this': 0.0,\n",
              "             'book': 0.0,\n",
              "             'will': 0.0,\n",
              "             'sit': 0.0})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_model[('<s>',)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bguObIDR-Orf",
        "outputId": "63566bc0-493b-49b0-9b7e-654fccd815fd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'called': 0.0,\n",
              "             'things': 0.0,\n",
              "             'oh': 0.018292682926829267,\n",
              "             'many': 0.0,\n",
              "             'then': 0.006097560975609756,\n",
              "             'does': 0.0,\n",
              "             'these': 0.012195121951219513,\n",
              "             'with': 0.0,\n",
              "             'fun': 0.0,\n",
              "             'would': 0.006097560975609756,\n",
              "             'mother': 0.0,\n",
              "             'bird': 0.0,\n",
              "             'story': 0.0,\n",
              "             'it': 0.012195121951219513,\n",
              "             'near': 0.0,\n",
              "             'nook': 0.0,\n",
              "             'glad': 0.0,\n",
              "             'never': 0.0,\n",
              "             'is': 0.0,\n",
              "             'girls': 0.0,\n",
              "             'a': 0.018292682926829267,\n",
              "             'sleep': 0.0,\n",
              "             'sits': 0.0,\n",
              "             'like': 0.0,\n",
              "             'they': 0.024390243902439025,\n",
              "             'his': 0.0,\n",
              "             'teeth': 0.0,\n",
              "             'something': 0.0,\n",
              "             'hook': 0.0,\n",
              "             'think': 0.0,\n",
              "             'comb': 0.006097560975609756,\n",
              "             'funny': 0.018292682926829267,\n",
              "             'moon': 0.0,\n",
              "             'quiet': 0.0,\n",
              "             'go': 0.0,\n",
              "             'gack': 0.0,\n",
              "             'ride': 0.0,\n",
              "             'what': 0.018292682926829267,\n",
              "             'up': 0.0,\n",
              "             'feet': 0.0,\n",
              "             'made': 0.0,\n",
              "             'seven': 0.0,\n",
              "             'yop': 0.0,\n",
              "             'wet': 0.0,\n",
              "             'your': 0.0,\n",
              "             'fish': 0.0,\n",
              "             'have': 0.006097560975609756,\n",
              "             'four': 0.0,\n",
              "             'ear': 0.0,\n",
              "             'saw': 0.0,\n",
              "             'car': 0.0,\n",
              "             'on': 0.024390243902439025,\n",
              "             'put': 0.0,\n",
              "             'zans': 0.0,\n",
              "             'should': 0.006097560975609756,\n",
              "             'fly': 0.0,\n",
              "             'anything': 0.0,\n",
              "             'grow': 0.0,\n",
              "             'socks': 0.0,\n",
              "             'gone': 0.0,\n",
              "             'shoe': 0.0,\n",
              "             'we': 0.04878048780487805,\n",
              "             'pets': 0.0,\n",
              "             'if': 0.024390243902439025,\n",
              "             'ned': 0.0,\n",
              "             'light': 0.0,\n",
              "             'and': 0.036585365853658534,\n",
              "             'he': 0.03048780487804878,\n",
              "             'fat': 0.0,\n",
              "             'big': 0.0,\n",
              "             'took': 0.0,\n",
              "             'right': 0.0,\n",
              "             'over': 0.0,\n",
              "             'pink': 0.0,\n",
              "             'again': 0.006097560975609756,\n",
              "             'why': 0.012195121951219513,\n",
              "             'i': 0.10365853658536585,\n",
              "             'my': 0.036585365853658534,\n",
              "             'bet': 0.0,\n",
              "             'good': 0.0,\n",
              "             'sad': 0.0,\n",
              "             'jump': 0.0,\n",
              "             'to': 0.0,\n",
              "             'cow': 0.0,\n",
              "             'know': 0.0,\n",
              "             'way': 0.0,\n",
              "             'tell': 0.006097560975609756,\n",
              "             'every': 0.006097560975609756,\n",
              "             'yellow': 0.0,\n",
              "             'work': 0.0,\n",
              "             'did': 0.024390243902439025,\n",
              "             'wish': 0.0,\n",
              "             'five': 0.006097560975609756,\n",
              "             'where': 0.006097560975609756,\n",
              "             'can': 0.006097560975609756,\n",
              "             'kind': 0.0,\n",
              "             'as': 0.006097560975609756,\n",
              "             'comes': 0.0,\n",
              "             'hand': 0.0,\n",
              "             'fingers': 0.0,\n",
              "             'grows': 0.0,\n",
              "             'day': 0.0,\n",
              "             'hello': 0.018292682926829267,\n",
              "             'upon': 0.0,\n",
              "             'lot': 0.0,\n",
              "             'head': 0.0,\n",
              "             'cats': 0.0,\n",
              "             'foot': 0.0,\n",
              "             'only': 0.0,\n",
              "             'home': 0.0,\n",
              "             'make': 0.0,\n",
              "             'slow': 0.0,\n",
              "             'ca': 0.0,\n",
              "             'back': 0.0,\n",
              "             'black': 0.006097560975609756,\n",
              "             'well': 0.006097560975609756,\n",
              "             'far': 0.0,\n",
              "             'all': 0.018292682926829267,\n",
              "             'gump': 0.0,\n",
              "             'box': 0.0,\n",
              "             '<s>': 0.0,\n",
              "             'told': 0.0,\n",
              "             'thing': 0.0,\n",
              "             'fear': 0.0,\n",
              "             'thin': 0.0,\n",
              "             'see': 0.0,\n",
              "             'so': 0.03048780487804878,\n",
              "             'me': 0.0,\n",
              "             'at': 0.012195121951219513,\n",
              "             'eight': 0.0,\n",
              "             'has': 0.0,\n",
              "             'am': 0.0,\n",
              "             'hear': 0.0,\n",
              "             'mouse': 0.0,\n",
              "             'ying': 0.0,\n",
              "             'joe': 0.0,\n",
              "             'him': 0.0,\n",
              "             'you': 0.012195121951219513,\n",
              "             'blue': 0.006097560975609756,\n",
              "             'top': 0.0,\n",
              "             'was': 0.0,\n",
              "             'said': 0.0,\n",
              "             'from': 0.03048780487804878,\n",
              "             'today': 0.006097560975609756,\n",
              "             'littlecar': 0.0,\n",
              "             'bed': 0.0,\n",
              "             'walked': 0.0,\n",
              "             'goodbye': 0.0,\n",
              "             'get': 0.0,\n",
              "             'ish': 0.0,\n",
              "             'just': 0.006097560975609756,\n",
              "             'very': 0.0,\n",
              "             'hat': 0.0,\n",
              "             '!': 0.0,\n",
              "             'gold': 0.0,\n",
              "             'cold': 0.0,\n",
              "             'that': 0.0,\n",
              "             'too': 0.0,\n",
              "             'wave': 0.0,\n",
              "             'about': 0.0,\n",
              "             'everywhere': 0.0,\n",
              "             'zeds': 0.0,\n",
              "             'them': 0.0,\n",
              "             '.': 0.0,\n",
              "             'the': 0.012195121951219513,\n",
              "             'yink': 0.0,\n",
              "             'bad': 0.0,\n",
              "             'ten': 0.0,\n",
              "             'cut': 0.0,\n",
              "             'wink': 0.0,\n",
              "             'swish': 0.0,\n",
              "             'ever': 0.0,\n",
              "             'wump': 0.0,\n",
              "             'not': 0.006097560975609756,\n",
              "             'of': 0.0,\n",
              "             'sings': 0.0,\n",
              "             'likes': 0.0,\n",
              "             'clark': 0.0,\n",
              "             'when': 0.012195121951219513,\n",
              "             '``': 0.0,\n",
              "             'night': 0.0,\n",
              "             'need': 0.0,\n",
              "             '?': 0.0,\n",
              "             'their': 0.006097560975609756,\n",
              "             ':': 0.0,\n",
              "             'left': 0.0,\n",
              "             'long': 0.0,\n",
              "             'ring': 0.0,\n",
              "             'read': 0.0,\n",
              "             'no': 0.0,\n",
              "             'heads': 0.0,\n",
              "             'bike': 0.0,\n",
              "             'pet': 0.0,\n",
              "             'mr.': 0.006097560975609756,\n",
              "             'more': 0.0,\n",
              "             'but': 0.024390243902439025,\n",
              "             'dad': 0.0,\n",
              "             'ink': 0.0,\n",
              "             'yell': 0.0,\n",
              "             'pop': 0.0,\n",
              "             'some': 0.042682926829268296,\n",
              "             'haircut': 0.0,\n",
              "             'call': 0.0,\n",
              "             'our': 0.006097560975609756,\n",
              "             'cans': 0.0,\n",
              "             'here': 0.006097560975609756,\n",
              "             'brush': 0.006097560975609756,\n",
              "             'gox': 0.0,\n",
              "             'tomorrow': 0.006097560975609756,\n",
              "             'mike': 0.006097560975609756,\n",
              "             'there': 0.006097560975609756,\n",
              "             'look': 0.0,\n",
              "             'bump': 0.006097560975609756,\n",
              "             'house': 0.0,\n",
              "             'hop': 0.012195121951219513,\n",
              "             'run': 0.0,\n",
              "             'hold': 0.0,\n",
              "             'new': 0.0,\n",
              "             'one': 0.012195121951219513,\n",
              "             'name': 0.0,\n",
              "             'star': 0.0,\n",
              "             'kite': 0.0,\n",
              "             'us': 0.0,\n",
              "             'live': 0.0,\n",
              "             'hills': 0.0,\n",
              "             'let': 0.0,\n",
              "             'high': 0.0,\n",
              "             'zeep': 0.0,\n",
              "             'please': 0.0,\n",
              "             'dish': 0.0,\n",
              "             'hair': 0.0,\n",
              "             'six': 0.0,\n",
              "             'dear': 0.0,\n",
              "             'try': 0.0,\n",
              "             'finger': 0.0,\n",
              "             'three': 0.0,\n",
              "             \"n't\": 0.0,\n",
              "             'how': 0.012195121951219513,\n",
              "             'open': 0.0,\n",
              "             'fast': 0.0,\n",
              "             \"''\": 0.0,\n",
              "             'town': 0.0,\n",
              "             'for': 0.0,\n",
              "             'ask': 0.0,\n",
              "             'man': 0.0,\n",
              "             'another': 0.0,\n",
              "             'had': 0.0,\n",
              "             'take': 0.0,\n",
              "             'milk': 0.0,\n",
              "             'time': 0.0,\n",
              "             'game': 0.0,\n",
              "             'are': 0.0,\n",
              "             'be': 0.0,\n",
              "             'do': 0.006097560975609756,\n",
              "             ',': 0.0,\n",
              "             'two': 0.0,\n",
              "             'who': 0.018292682926829267,\n",
              "             'out': 0.0,\n",
              "             'in': 0.006097560975609756,\n",
              "             'down': 0.0,\n",
              "             'yes': 0.006097560975609756,\n",
              "             '...': 0.0,\n",
              "             'sheep': 0.0,\n",
              "             'drink': 0.0,\n",
              "             'wire': 0.0,\n",
              "             'walk': 0.0,\n",
              "             'hump': 0.0,\n",
              "             'come': 0.006097560975609756,\n",
              "             ';': 0.0,\n",
              "             'nine': 0.0,\n",
              "             'old': 0.0,\n",
              "             'by': 0.006097560975609756,\n",
              "             'now': 0.0,\n",
              "             'sun': 0.0,\n",
              "             'red': 0.0,\n",
              "             'off': 0.0,\n",
              "             'sing': 0.0,\n",
              "             'sticks': 0.0,\n",
              "             'may': 0.0,\n",
              "             'stick': 0.0,\n",
              "             'low': 0.0,\n",
              "             'eleven': 0.006097560975609756,\n",
              "             'hot': 0.0,\n",
              "             'say': 0.018292682926829267,\n",
              "             'cook': 0.0,\n",
              "             'must': 0.0,\n",
              "             'little': 0.0,\n",
              "             'play': 0.0,\n",
              "             'this': 0.03048780487804878,\n",
              "             'book': 0.0,\n",
              "             'will': 0.018292682926829267,\n",
              "             'sit': 0.0})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, context):\n",
        "    \"\"\"Returns a token sampled from the `model` assuming the `context`\"\"\"\n",
        "    distribution = model[context]\n",
        "    prob_remaining = random.random()\n",
        "    for token, prob in sorted(distribution.items()):\n",
        "        if prob_remaining < prob:\n",
        "            return token\n",
        "        else:\n",
        "            prob_remaining -= prob\n",
        "    raise ValueError"
      ],
      "metadata": {
        "id": "tvfswkUxS9Mp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_sequence(model, start_context, count=100):\n",
        "    \"\"\"Returns a sequence of `count` tokens sampled successively\n",
        "       from the `model` *following the `start_context`*.\n",
        "       The length of the returned list should be `count+len(start_context)`.\n",
        "    \"\"\"\n",
        "    sequence = []\n",
        "    random.seed(SEED)  # for reproducibility, do not change\n",
        "    changed_start_context = list(start_context)\n",
        "    for i in range(count):\n",
        "        next_token = sample(model, tuple(start_context))\n",
        "        sequence.append(next_token)\n",
        "        changed_start_context.append(next_token)\n",
        "    return sequence"
      ],
      "metadata": {
        "id": "wxJ-6ZJdypr9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(postprocess(sample_sequence(unigram_model, ())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHC1i6tqzdO3",
        "outputId": "b1faca38-380d-4b3b-d214-d120b463448f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "would fast ! up what i me . pink \n",
            ", see at is is \n",
            "\n",
            ". ! good with , him fun in . i a hot like go bed \n",
            "we five hello , have ! . game by , i \n",
            "house ... what pop wink . and , a so \n",
            "\n",
            "will \n",
            "ten , can bad ask some get saw funny some to far said some all . \n",
            "every . hello take good some likes like at near some \n",
            "town a \n",
            "the not should fingers . comb , a ,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probability metric"
      ],
      "metadata": {
        "id": "fB7iB34Gzonm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def probability(tokens, model, n):\n",
        "    \"\"\"Returns the probability of a sequence of `tokens` according to an\n",
        "       `n`-gram `model`\n",
        "    \"\"\"\n",
        "    score = 1.0\n",
        "    context = tokens[0:n-1]\n",
        "    # Ignores the scores of the first n-1 tokens\n",
        "    for token in tokens[n-1:]:\n",
        "        # print(f\"heeeeeey in {n}\")\n",
        "        prob = model[tuple(context)][token]\n",
        "        if prob:\n",
        "          # print(f\"this isthe currecnt prob in the model {prob}\")\n",
        "          score *= prob\n",
        "        # print(f\"this isthe currecnt score{score}\")\n",
        "        context = (context + [token])[1:]\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "sF46CRnazhM1"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_tokens[:10])\n",
        "print(f\"Test probability - unigram: {probability(test_tokens, unigram_model, 1):6e}\\n\"\n",
        "      f\"Test probability -  bigram: {probability(test_tokens, bigram_model, 2):6e}\\n\"\n",
        "      f\"Test probability - trigram: {probability(test_tokens, trigram_model, 3):6e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkc9xq9rzsAA",
        "outputId": "a48859d2-b007-41d5-c61a-346a36cc6694"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'black', 'fish', ',', 'blue', 'fish', ',', 'old', 'fish', ',']\n",
            "Test probability - unigram: 9.307809e-311\n",
            "Test probability -  bigram: 1.349544e-78\n",
            "Test probability - trigram: 3.675613e-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The negative log probability metric"
      ],
      "metadata": {
        "id": "x1L1CZs-P8ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def neglogprob(tokens, model, n):\n",
        "    \"\"\"Returns the negative log probability of a sequence of `tokens`\n",
        "       according to an `n`-gram `model`\n",
        "    \"\"\"\n",
        "    temp_prob = probability(tokens, model, n)\n",
        "    if temp_prob == 0:\n",
        "      return math.inf\n",
        "    else:\n",
        "      return -math.log2(temp_prob)"
      ],
      "metadata": {
        "id": "3gZkkAaHztwM"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_test_nlp = neglogprob(test_tokens, unigram_model, 1)\n",
        "bigram_test_nlp = neglogprob(test_tokens, bigram_model, 2)\n",
        "trigram_test_nlp = neglogprob(test_tokens, trigram_model, 3)\n",
        "\n",
        "print(f\"Test neglogprob - unigram: {unigram_test_nlp:6f}\\n\"\n",
        "      f\"Test neglogprob -  bigram: {bigram_test_nlp:6f}\\n\"\n",
        "      f\"Test neglogprob - trigram: {trigram_test_nlp:6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuBGct_5z0F9",
        "outputId": "cb9929dc-628b-494a-e681-e39c80908bfe"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test neglogprob - unigram: 1029.901196\n",
            "Test neglogprob -  bigram: 258.677919\n",
            "Test neglogprob - trigram: 51.272865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Test probability - unigram: {2 ** (-unigram_test_nlp):6e}\\n\"\n",
        "      f\"Test probability -  bigram: {2 ** (-bigram_test_nlp):6e}\\n\"\n",
        "      f\"Test probability - trigram: {2 ** (-trigram_test_nlp):6e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeDMK72Pz0-0",
        "outputId": "591b9d2e-1f1c-4669-a284-8829a2330307"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test probability - unigram: 9.307809e-311\n",
            "Test probability -  bigram: 1.349544e-78\n",
            "Test probability - trigram: 3.675613e-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The perplexity metric"
      ],
      "metadata": {
        "id": "RTNn6mkJQDm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity(tokens, model, n):\n",
        "    \"\"\"Returns the perplexity of a sequence of `tokens` according to an\n",
        "       `n`-gram `model`\n",
        "    \"\"\"\n",
        "    nlprob = neglogprob(tokens, model, n)\n",
        "    return 2**(nlprob/len(tokens[n-1:]))"
      ],
      "metadata": {
        "id": "LMNg93syz3M7"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Test perplexity - unigram: {perplexity(test_tokens, unigram_model, 1):.3f}\\n\"\n",
        "      f\"Test perplexity -  bigram: {perplexity(test_tokens, bigram_model, 2):.3f}\\n\"\n",
        "      f\"Test perplexity - trigram: {perplexity(test_tokens, trigram_model, 3):.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXJUxq-dz5Hu",
        "outputId": "f0b12d8e-f942-4172-a03d-46bc92a7baa0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test perplexity - unigram: 81.992\n",
            "Test perplexity -  bigram: 3.046\n",
            "Test perplexity - trigram: 1.249\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add-Delta smoothing"
      ],
      "metadata": {
        "id": "1UFkq2YsQPXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ngram_model_smoothed(ngram_counts):\n",
        "    \"\"\"Returns an n-gram probability model calculated by normalizing the\n",
        "       provided `ngram-counts` dictionary\n",
        "    \"\"\"\n",
        "    delta = 1\n",
        "    V = []\n",
        "    normalized_counts = defaultdict(lambda: defaultdict(int))\n",
        "    for key, value in ngram_counts.items():\n",
        "      for subsequent, count in value.items():\n",
        "        normalized_counts[key][subsequent] = count\n",
        "        V.append(subsequent)\n",
        "    V = set(V)\n",
        "    vocab_size = len(V)\n",
        "    V = len(ngram_counts.items())\n",
        "    for value in normalized_counts.values():\n",
        "      total_count = sum(value.values())\n",
        "      for subsequent, count in value.items():\n",
        "        value[subsequent] = (count + delta) / (total_count + (delta * vocab_size))if total_count != 0 else 0\n",
        "\n",
        "    return normalized_counts"
      ],
      "metadata": {
        "id": "IVUTCgUJz6NP"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_model_smoothed = ngram_model_smoothed(unigram_counts)\n",
        "bigram_model_smoothed = ngram_model_smoothed(bigram_counts)\n",
        "trigram_model_smoothed = ngram_model_smoothed(trigram_counts)\n",
        "\n",
        "print(f\"Test perplexity - unigram: {perplexity(test_tokens, unigram_model_smoothed, 1):.3f}\\n\"\n",
        "      f\"Test perplexity -  bigram: {perplexity(test_tokens, bigram_model_smoothed, 2):.3f}\\n\"\n",
        "      f\"Test perplexity - trigram: {perplexity(test_tokens, trigram_model_smoothed, 3):.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y57m1vq_uSV",
        "outputId": "2ad1eac9-81d2-42c3-d752-aa007a0dc246"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test perplexity - unigram: 81.739\n",
            "Test perplexity -  bigram: 79.678\n",
            "Test perplexity - trigram: 13.293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ngram_model_smoothed_second(ngram_counts):\n",
        "    \"\"\"Returns an n-gram probability model calculated by normalizing the\n",
        "       provided `ngram-counts` dictionary\n",
        "    \"\"\"\n",
        "    delta = 2\n",
        "    V = []\n",
        "    normalized_counts = defaultdict(lambda: defaultdict(int))\n",
        "    for key, value in ngram_counts.items():\n",
        "      for subsequent, count in value.items():\n",
        "        normalized_counts[key][subsequent] = count\n",
        "        V.append(subsequent)\n",
        "    V = set(V)\n",
        "    vocab_size = len(V)\n",
        "    V = len(ngram_counts.items())\n",
        "    for value in normalized_counts.values():\n",
        "      total_count = sum(value.values())\n",
        "      for subsequent, count in value.items():\n",
        "        value[subsequent] = (count + delta) / (total_count + (delta * vocab_size))if total_count != 0 else 0\n",
        "\n",
        "    return normalized_counts"
      ],
      "metadata": {
        "id": "Pv_Eea2E_wkc"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_model_smoothed = ngram_model_smoothed_second(unigram_counts)\n",
        "bigram_model_smoothed = ngram_model_smoothed_second(bigram_counts)\n",
        "trigram_model_smoothed = ngram_model_smoothed_second(trigram_counts)\n",
        "\n",
        "print(f\"Test perplexity - unigram: {perplexity(test_tokens, unigram_model_smoothed, 1):.3f}\\n\"\n",
        "      f\"Test perplexity -  bigram: {perplexity(test_tokens, bigram_model_smoothed, 2):.3f}\\n\"\n",
        "      f\"Test perplexity - trigram: {perplexity(test_tokens, trigram_model_smoothed, 3):.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK2hEs0BFrwg",
        "outputId": "1deccb16-26e5-4b4f-db5e-aa2dc4f9f00d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test perplexity - unigram: 83.611\n",
            "Test perplexity -  bigram: 94.465\n",
            "Test perplexity - trigram: 14.299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Knesser-Nay smoothing"
      ],
      "metadata": {
        "id": "bY4pFVsiQUyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def knesser_nay_smoothing(ngram_counts):\n",
        "    \"\"\"Returns an n-gram probability model calculated using Kneser-Ney smoothing\n",
        "       on the provided `ngram-counts` dictionary.\n",
        "    \"\"\"\n",
        "    normalized_counts = defaultdict(lambda: defaultdict(int))\n",
        "    context_counts = defaultdict(int)\n",
        "\n",
        "    for context, target_counts in ngram_counts.items():\n",
        "        total_count = sum(target_counts.values())\n",
        "\n",
        "        # Handle zero total_count\n",
        "        if total_count == 0:\n",
        "            vocab_size = len(target_counts)\n",
        "            discount = 0.75 / (vocab_size + 1)\n",
        "            for target in target_counts:\n",
        "                normalized_counts[context][target] = discount\n",
        "        else:\n",
        "            for target, count in target_counts.items():\n",
        "                normalized_counts[context][target] = max(count - 0.75, 0) / total_count\n",
        "                context_counts[context] += normalized_counts[context][target]\n",
        "\n",
        "    for context, target_counts in normalized_counts.items():\n",
        "        total_context_count = context_counts[context]\n",
        "\n",
        "        # Handle zero total_context_count\n",
        "        if total_context_count == 0:\n",
        "            vocab_size = len(target_counts)\n",
        "            discount = 0.75 / (vocab_size + 1)\n",
        "            for target in target_counts:\n",
        "                normalized_counts[context][target] += discount\n",
        "        else:\n",
        "            discount = 0.75 / total_context_count\n",
        "            for target in target_counts:\n",
        "                normalized_counts[context][target] += discount\n",
        "\n",
        "    return normalized_counts\n"
      ],
      "metadata": {
        "id": "fRO5u8mhMkBb"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_model_smoothed = knesser_nay_smoothing(unigram_counts)\n",
        "bigram_model_smoothed = knesser_nay_smoothing(bigram_counts)\n",
        "trigram_model_smoothed = knesser_nay_smoothing(trigram_counts)\n",
        "\n",
        "print(f\"Test perplexity - unigram: {perplexity(test_tokens, unigram_model_smoothed, 1):.3f}\\n\"\n",
        "      f\"Test perplexity -  bigram: {perplexity(test_tokens, bigram_model_smoothed, 2):.3f}\\n\"\n",
        "      f\"Test perplexity - trigram: {perplexity(test_tokens, trigram_model_smoothed, 3):.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM81c_2bM4ip",
        "outputId": "af4b796b-164d-49b1-8aef-dcaf520ef8b8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test perplexity - unigram: 1.112\n",
            "Test perplexity -  bigram: 0.610\n",
            "Test perplexity - trigram: 4.339\n"
          ]
        }
      ]
    }
  ]
}